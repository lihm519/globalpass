#!/usr/bin/env python3
"""
Airalo çˆ¬è™« V3 - è§£æ JavaScript ä¸­çš„ JSON æ•°æ®
"""

import json
import logging
import re
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from typing import List, Dict

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# æ±‡ç‡
EUR_TO_USD = 1.1715

def scrape_airalo_country(country_name: str, airalo_slug: str) -> List[Dict]:
    """ä» Airalo å®˜ç½‘æŠ“å–å•ä¸ªå›½å®¶çš„æ•°æ®"""
    try:
        url = f"https://www.airalo.com/{airalo_slug}-esim"
        
        logger.info(f"ğŸŒ æ­£åœ¨æŠ“å– Airalo - {country_name}...")
        
        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=15)
        
        if response.status_code != 200:
            logger.warning(f"âŒ Airalo {country_name}: HTTP {response.status_code}")
            return []
        
        soup = BeautifulSoup(response.content, 'html.parser')
        packages = []
        
        # æŸ¥æ‰¾åŒ…å« JSON æ•°æ®çš„ script æ ‡ç­¾
        script_tags = soup.find_all('script', string=lambda text: text and '"title":' in text and 'GB' in text)
        
        for script in script_tags:
            script_text = script.string
            
            # æå–æ‰€æœ‰å¥—é¤åç§°: "1 GB - 3 Days"
            title_matches = re.findall(r'"(\d+ GB - \d+ Days)"', script_text)
            
            # åŒ¹é…ä»·æ ¼: "4.00 â‚¬"
            price_matches = re.findall(r'"([\d.]+) â‚¬"', script_text)
            
            logger.debug(f"   æ‰¾åˆ° {len(title_matches)} ä¸ªæ ‡é¢˜, {len(price_matches)} ä¸ªä»·æ ¼")
            
            # åŒ¹é…å¥—é¤åç§°å’Œä»·æ ¼ï¼ˆæ¯ä¸ªå¥—é¤æœ‰ 2 ä¸ªä»·æ ¼ï¼Œå–ç¬¬ä¸€ä¸ªï¼‰
            for i, title in enumerate(title_matches):
                # è§£ææ ‡é¢˜: "1 GB - 3 Days"
                standard_match = re.match(r'^(\d+)\s*GB\s*-\s*(\d+)\s*Days?$', title, re.IGNORECASE)
                
                if standard_match and i * 2 < len(price_matches):
                    data_amount = standard_match.group(1)
                    validity_days = standard_match.group(2)
                    validity = f"{validity_days} Day" if validity_days == "1" else f"{validity_days} Days"
                    price_eur = float(price_matches[i * 2])  # æ¯ä¸ªå¥—é¤æœ‰ 2 ä¸ªä»·æ ¼
                    price_usd = round(price_eur * EUR_TO_USD, 2)
                    
                    package = {
                        "provider": "Airalo",
                        "country": country_name,
                        "plan_name": f"{country_name} {data_amount}GB {validity}",
                        "data_type": "Data",
                        "data_amount": f"{data_amount}GB",
                        "validity": validity,
                        "price": price_usd,
                        "network": "Local Operators",
                        "link": url,
                        "raw_data": json.dumps({
                            "original_price": price_eur,
                            "original_currency": "EUR",
                            "usd_price": price_usd,
                            "currency": "USD",
                            "data": f"{data_amount}GB",
                            "validity": validity,
                        }),
                        "last_checked": datetime.utcnow().isoformat(),
                    }
                    packages.append(package)
                    logger.debug(f"   âœ… {data_amount}GB {validity} â‚¬{price_eur} â†’ ${price_usd}")
        
        logger.info(f"âœ… Airalo {country_name}: è·å– {len(packages)} ä¸ªå¥—é¤")
        return packages
        
    except Exception as e:
        logger.error(f"âŒ Airalo {country_name}: {e}")
        return []

if __name__ == "__main__":
    # æµ‹è¯•
    packages = scrape_airalo_country("Japan", "japan")
    print(f"\nè·å– {len(packages)} ä¸ªå¥—é¤:")
    for pkg in packages:
        print(f"  - {pkg['plan_name']}: ${pkg['price']}")
