#!/usr/bin/env python3
"""
GlobalPass - é€šç”¨çˆ¬è™«æ ¸å¿ƒç³»ç»Ÿï¼ˆç½‘é¡µæŠ“å–ç‰ˆï¼‰
é˜¶æ®µäºŒï¼šè‡ªåŠ¨åŒ–ä¾›è´§ç³»ç»Ÿ
åŠŸèƒ½ï¼š
- ä» Airalo å®˜ç½‘ç½‘é¡µæŠ“å–çœŸå®æ•°æ®
- ä» Nomad å®˜ç½‘ç½‘é¡µæŠ“å–çœŸå®æ•°æ®
- è´§å¸è½¬æ¢ï¼ˆEUR â†’ USDï¼‰
- æ— é™æµé‡è¯†åˆ«
- æœ‰æ•ˆæœŸæ¸…æ´—
- Upsert å…¥åº“
"""
import json
import requests
import logging
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional
import re
from bs4 import BeautifulSoup

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Supabase é…ç½®
SUPABASE_URL = "https://mzodnvjtlujvvwfnpcyb.supabase.co"
SERVICE_ROLE_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Im16b2Rudmp0bHVqdnZ3Zm5wY3liIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc2NzU0MDk4NiwiZXhwIjoyMDgzMTE2OTg2fQ.gr-5J22EhV08PLghNcoS8o5lUFjaEyby21MwE-35ENs"

# æ³¨æ„ï¼šä¸å†ä½¿ç”¨æ±‡ç‡è½¬æ¢ï¼Œç›´æ¥ä½¿ç”¨æä¾›å•†çš„åŸå§‹ä»·æ ¼

class UniversalScraper:
    """é€šç”¨çˆ¬è™«ç±» - æ”¯æŒ Airalo å’Œ Nomad"""
    
    def __init__(self):
        self.supabase_headers = {
            "apikey": SERVICE_ROLE_KEY,
            "Authorization": f"Bearer {SERVICE_ROLE_KEY}",
            "Content-Type": "application/json",
        }
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        })
        self.stats = {
            "airalo_scraped": 0,
            "nomad_scraped": 0,
            "upsert_success": 0,
            "upsert_error": 0,
        }
    
    def load_countries(self) -> List[Dict]:
        """åŠ è½½å›½å®¶é…ç½®"""
        config_file = Path(__file__).parent.parent / "config" / "countries.json"
        
        if not config_file.exists():
            logger.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
            return []
        
        with open(config_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    # å·²ç§»é™¤æ±‡ç‡è½¬æ¢å‡½æ•°ï¼Œä½¿ç”¨æä¾›å•†åŸå§‹ä»·æ ¼
    
    def scrape_airalo_country(self, country: Dict) -> List[Dict]:
        """ä» Airalo å®˜ç½‘æŠ“å–å•ä¸ªå›½å®¶çš„æ•°æ®"""
        try:
            url = f"https://www.airalo.com/{country['airalo_slug']}-esim"
            
            logger.info(f"ğŸŒ æ­£åœ¨æŠ“å– Airalo - {country['name']}...")
            
            response = self.session.get(url, timeout=15)
            
            if response.status_code != 200:
                logger.warning(f"âŒ Airalo {country['name']}: HTTP {response.status_code}")
                return []
            
            soup = BeautifulSoup(response.content, 'html.parser')
            packages = []
            
            # æŸ¥æ‰¾æ‰€æœ‰å¥—é¤é“¾æ¥
            package_links = soup.find_all('a')
            
            validity_map = {}
            current_validity = "7 Days"  # é»˜è®¤æœ‰æ•ˆæœŸ
            
            for link in package_links:
                text = link.get_text(strip=True)
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆæœŸæ ‡ç­¾ï¼ˆå¦‚ "3 days", "7 days" ç­‰ï¼‰
                if re.match(r'^\d+\s*days?$', text, re.IGNORECASE):
                    current_validity = text.replace('days', 'Days').replace('day', 'Day')
                    logger.debug(f"   ğŸ“… æ£€æµ‹åˆ°æœ‰æ•ˆæœŸ: {current_validity}")
                    continue
                
                # è§£ææ ‡å‡†å¥—é¤: "1GB4.00 â‚¬" æˆ– "1GB4.00â‚¬" æ ¼å¼
                standard_match = re.match(r'^(\d+)\s*GB([\d.]+)\s*â‚¬$', text)
                
                if standard_match:
                    data_amount = standard_match.group(1)
                    price = float(standard_match.group(2))
                    
                    package = {
                        "provider": "Airalo",
                        "country": country['name'],
                        "plan_name": f"{country['name']} {data_amount}GB {current_validity}",
                        "data_type": "Data",
                        "data_amount": f"{data_amount}GB",
                        "validity": current_validity,
                        "price": price,
                        "network": "Local Operators",
                        "link": f"https://www.airalo.com/{country['airalo_slug']}-esim",
                        "raw_data": json.dumps({
                            "original_price": price,
                            "currency": "EUR",
                            "data": f"{data_amount}GB",
                            "validity": current_validity,
                        }),
                        "last_checked": datetime.utcnow().isoformat(),
                    }
                    packages.append(package)
                    logger.debug(f"   âœ… æ ‡å‡†å¥—é¤: {data_amount}GB â‚¬{price}")
                
                # è§£ææ— é™æµé‡å¥—é¤: "Unlimited7.50 â‚¬" æˆ– "UnlimitedData7.50 â‚¬"
                unlimited_match = re.match(r'^Unlimited(?:Data)?([\d.]+)\s*â‚¬$', text)
                
                if unlimited_match:
                    price = float(unlimited_match.group(1))
                    
                    package = {
                        "provider": "Airalo",
                        "country": country['name'],
                        "plan_name": f"{country['name']} Unlimited {current_validity}",
                        "data_type": "Unlimited",
                        "data_amount": "Unlimited",
                        "validity": current_validity,
                        "price": price,
                        "network": "Local Operators",
                        "link": f"https://www.airalo.com/{country['airalo_slug']}-esim",
                        "raw_data": json.dumps({
                            "original_price": price,
                            "currency": "EUR",
                            "data": "Unlimited",
                            "validity": current_validity,
                        }),
                        "last_checked": datetime.utcnow().isoformat(),
                    }
                    packages.append(package)
                    logger.debug(f"   âœ… æ— é™å¥—é¤: Unlimited â‚¬{price}")
            
            logger.info(f"âœ… Airalo {country['name']}: è·å– {len(packages)} ä¸ªå¥—é¤")
            self.stats["airalo_scraped"] += len(packages)
            return packages
            
        except Exception as e:
            logger.error(f"âŒ Airalo {country['name']} é”™è¯¯: {str(e)[:100]}")
            return []
    
    def scrape_nomad_country(self, country: Dict) -> List[Dict]:
        """ä» Nomad å®˜ç½‘æŠ“å–å•ä¸ªå›½å®¶çš„æ•°æ®"""
        try:
            # æ„å»º Nomad URL
            nomad_slug = country['nomad_slug'].replace('_', '-')
            url = f"https://www.getnomad.app/{nomad_slug}-esim"
            
            logger.info(f"ğŸŒ æ­£åœ¨æŠ“å– Nomad - {country['name']}...")
            
            response = self.session.get(url, timeout=15)
            
            if response.status_code != 200:
                logger.warning(f"âŒ Nomad {country['name']}: HTTP {response.status_code}")
                return []
            
            soup = BeautifulSoup(response.content, 'html.parser')
            packages = []
            
            # æŸ¥æ‰¾æ‰€æœ‰å¥—é¤ <li> å…ƒç´ 
            plan_items = soup.find_all('li')
            
            for item in plan_items:
                text = item.get_text(strip=True)
                
                # è·³è¿‡ä¸åŒ…å« SGD æˆ– EUR çš„é¡¹ç›®
                if 'SGD' not in text and 'EUR' not in text:
                    continue
                
                # æå–æ•°æ®é‡ã€æœ‰æ•ˆæœŸå’Œä»·æ ¼
                # æ ¼å¼: "Plan Details1 GBFor 7 DAYSSGD5.14" æˆ– "Plan DetailsUnlimitedFor 3 DAYSSGD14.15"
                
                # ç§»é™¤ "Plan Details" å‰ç¼€
                text_clean = text.replace('Plan Details', '').strip()
                
                # åŒ¹é…æ•°æ®é‡
                data_match = re.search(r'(\d+)\s*GB|Unlimited', text_clean)
                if not data_match:
                    continue
                
                data_str = data_match.group(0)  # "1 GB" æˆ– "Unlimited"
                
                # åŒ¹é…æœ‰æ•ˆæœŸ
                validity_match = re.search(r'For\s+(\d+)\s*DAYS', text_clean, re.IGNORECASE)
                if not validity_match:
                    continue
                
                validity_days = validity_match.group(1)
                validity = f"{validity_days} Days"
                
                # åŒ¹é…ä»·æ ¼ (SGD æˆ– EUR)
                price_match = re.search(r'(SGD|EUR)([\d.]+)', text_clean)
                if not price_match:
                    continue
                
                currency = price_match.group(1)
                price_value = float(price_match.group(2))
                
                # ä½¿ç”¨åŸå§‹ä»·æ ¼ï¼Œä¸è¿›è¡Œæ±‡ç‡è½¬æ¢
                original_price = price_value
                
                # åˆ¤æ–­æ˜¯å¦æ˜¯æ— é™æµé‡
                is_unlimited = 'Unlimited' in data_str
                
                if is_unlimited:
                    data_amount = "Unlimited"
                    data_type = "Unlimited"
                else:
                    data_amount = data_str.replace(' GB', 'GB').strip()
                    data_type = "Data"
                
                package = {
                    "provider": "Nomad",
                    "country": country['name'],
                    "plan_name": f"{country['name']} {data_str.strip()} {validity}",
                    "data_type": data_type,
                    "data_amount": data_amount,
                    "validity": validity,
                    "price": original_price,
                    "network": "Local Operators",
                    "link": f"https://www.getnomad.app/{nomad_slug}-esim",
                    "raw_data": json.dumps({
                        "currency": currency,
                        "original_price": price_value,
                        "data": data_str.strip(),
                        "validity": validity,
                    }),
                    "last_checked": datetime.utcnow().isoformat(),
                }
                packages.append(package)
                logger.debug(f"   âœ… å¥—é¤: {data_str.strip()} {validity} {currency}{price_value}")
            
            logger.info(f"âœ… Nomad {country['name']}: è·å– {len(packages)} ä¸ªå¥—é¤")
            self.stats["nomad_scraped"] += len(packages)
            return packages
            
        except Exception as e:
            logger.error(f"âŒ Nomad {country['name']} é”™è¯¯: {str(e)[:100]}")
            return []
    
    def upsert_to_supabase(self, packages: List[Dict]) -> int:
        """Upsert æ•°æ®åˆ° Supabase"""
        if not packages:
            return 0
        
        success_count = 0
        
        for pkg in packages:
            try:
                url = f"{SUPABASE_URL}/rest/v1/esim_packages"
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                check_url = f"{url}?provider=eq.{pkg['provider']}&country=eq.{pkg['country']}&plan_name=eq.{pkg['plan_name']}"
                
                check_response = requests.get(
                    check_url,
                    headers=self.supabase_headers,
                    timeout=10
                )
                
                if check_response.status_code == 200 and check_response.json():
                    # å·²å­˜åœ¨ï¼Œæ‰§è¡Œæ›´æ–°
                    existing_id = check_response.json()[0]['id']
                    update_url = f"{url}?id=eq.{existing_id}"
                    
                    response = requests.patch(
                        update_url,
                        headers=self.supabase_headers,
                        json=pkg,
                        timeout=10
                    )
                else:
                    # ä¸å­˜åœ¨ï¼Œæ‰§è¡Œæ’å…¥
                    response = requests.post(
                        url,
                        headers=self.supabase_headers,
                        json=pkg,
                        timeout=10
                    )
                
                if response.status_code in [200, 201, 204]:
                    success_count += 1
                    logger.info(f"âœ… {pkg['provider']} - {pkg['country']} - {pkg['plan_name']}: å…¥åº“æˆåŠŸ (${pkg['price']})")
                else:
                    logger.warning(f"âš ï¸  {pkg['country']}: HTTP {response.status_code}")
                    logger.debug(f"   å“åº”: {response.text[:100]}")
                    self.stats["upsert_error"] += 1
                    
            except Exception as e:
                logger.error(f"âŒ Upsert é”™è¯¯: {str(e)[:100]}")
                self.stats["upsert_error"] += 1
        
        self.stats["upsert_success"] += success_count
        return success_count
    
    def run(self):
        """æ‰§è¡Œçˆ¬è™«"""
        print("\n" + "=" * 70)
        print("ğŸš€ GlobalPass - é€šç”¨çˆ¬è™«ç³»ç»Ÿå¯åŠ¨ (Airalo + Nomad)")
        print("=" * 70)
        
        countries = self.load_countries()
        
        if not countries:
            logger.error("âŒ æ— å¯ç”¨å›½å®¶é…ç½®")
            return 1
        
        logger.info(f"ğŸ“ ç›®æ ‡å›½å®¶: {len(countries)} ä¸ª")
        
        # éå†æ‰€æœ‰å›½å®¶
        for country in countries:
            logger.info(f"\n{'='*60}")
            logger.info(f"ğŸŒ å¤„ç†å›½å®¶: {country['name']}")
            logger.info(f"{'='*60}")
            
            # æŠ“å– Airalo æ•°æ®
            airalo_packages = self.scrape_airalo_country(country)
            if airalo_packages:
                self.upsert_to_supabase(airalo_packages)
            
            # æŠ“å– Nomad æ•°æ®
            nomad_packages = self.scrape_nomad_country(country)
            if nomad_packages:
                self.upsert_to_supabase(nomad_packages)
        
        # è¾“å‡ºç»Ÿè®¡
        print("\n" + "=" * 70)
        print("ğŸ“Š çˆ¬è™«æ‰§è¡Œç»Ÿè®¡")
        print("=" * 70)
        print(f"Airalo å¥—é¤: {self.stats['airalo_scraped']}")
        print(f"Nomad å¥—é¤: {self.stats['nomad_scraped']}")
        print(f"æ€»è®¡: {self.stats['airalo_scraped'] + self.stats['nomad_scraped']} ä¸ªå¥—é¤")
        print(f"Upsert æˆåŠŸ: {self.stats['upsert_success']}, å¤±è´¥: {self.stats['upsert_error']}")
        print("=" * 70)
        
        return 0

def main():
    scraper = UniversalScraper()
    return scraper.run()

if __name__ == "__main__":
    import sys
    sys.exit(main())
