"""
GlobalPass - æ··åˆçˆ¬è™«ç³»ç»Ÿï¼ˆä¿®å¤ç‰ˆï¼‰
åŠŸèƒ½ï¼š
- Airalo: Selenium + æ—§æ•°æ®å…œåº•
- Nomad: BeautifulSoupï¼ˆç¨³å®šå¯é ï¼‰
- æ•°æ®åº“å†™å…¥ä¿®å¤
"""
import json
import requests
import logging
import os
from datetime import datetime
from pathlib import Path
from typing import List, Dict
import re
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# åˆ›å»ºæ—¥å¿—ç›®å½•
log_dir = Path("logs")
log_dir.mkdir(exist_ok=True)

# é…ç½®æ—¥å¿—
log_file = log_dir / f"scraper_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Supabase é…ç½®
SUPABASE_URL = os.getenv("SUPABASE_URL", "https://mzodnvjtlujvvwfnpcyb.supabase.co")
SERVICE_ROLE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")

class UniversalScraper:
    """æ··åˆçˆ¬è™«ç±»"""
    
    def __init__(self):
        self.supabase_headers = {
            "apikey": SERVICE_ROLE_KEY,
            "Authorization": f"Bearer {SERVICE_ROLE_KEY}",
            "Content-Type": "application/json",
            "Prefer": "resolution=merge-duplicates"
        }
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        })
        self.stats = {
            "airalo_scraped": 0,
            "nomad_scraped": 0,
            "upsert_success": 0,
            "upsert_error": 0,
        }
        self.old_airalo_data = {}
        self.driver = None
    
    def load_countries(self) -> List[Dict]:
        """åŠ è½½å›½å®¶é…ç½®"""
        config_file = Path(__file__).parent.parent / "config" / "countries.json"
        
        if not config_file.exists():
            logger.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
            return []
        
        with open(config_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def load_old_airalo_data(self):
        """åŠ è½½æ—§çš„ Airalo æ•°æ®ä½œä¸ºå…œåº•"""
        try:
            data_file = Path(__file__).parent.parent / "public" / "data" / "esim-packages.json"
            if data_file.exists():
                with open(data_file, 'r', encoding='utf-8') as f:
                    all_data = json.load(f)
                    # æ•°æ®æ–‡ä»¶ç»“æ„: {"timestamp": ..., "all_packages": [...]}
                    packages = all_data.get("all_packages", [])
                    if not packages and isinstance(all_data, list):
                        # å…¼å®¹æ—§æ ¼å¼ï¼šç›´æ¥æ˜¯æ•°ç»„
                        packages = all_data
                    
                    # éå†æ‰€æœ‰å¥—é¤ï¼Œæå– Airalo æ•°æ®
                    for pkg in packages:
                        if isinstance(pkg, dict) and pkg.get("provider") == "Airalo":
                            country = pkg.get("country")
                            if country:
                                if country not in self.old_airalo_data:
                                    self.old_airalo_data[country] = []
                                self.old_airalo_data[country].append(pkg)
                    
                    total_packages = sum(len(pkgs) for pkgs in self.old_airalo_data.values())
                    logger.info(f"âœ… åŠ è½½æ—§æ•°æ®ï¼š{len(self.old_airalo_data)} ä¸ªå›½å®¶ï¼Œå…± {total_packages} ä¸ª Airalo å¥—é¤")
        except Exception as e:
            logger.warning(f"âš ï¸ æ— æ³•åŠ è½½æ—§æ•°æ®: {e}")
    
    def init_selenium(self):
        """åˆå§‹åŒ– Selenium WebDriver"""
        if self.driver:
            return
        
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        self.driver = webdriver.Chrome(options=chrome_options)
    
    def close_selenium(self):
        """å…³é—­ Selenium"""
        if self.driver:
            self.driver.quit()
            self.driver = None
    
    def scrape_airalo_country(self, country: Dict) -> List[Dict]:
        """ä½¿ç”¨ Selenium æŠ“å– Airaloï¼ˆå¤±è´¥åˆ™ä½¿ç”¨æ—§æ•°æ®ï¼‰"""
        try:
            self.init_selenium()
            url = f"https://www.airalo.com/{country['airalo_slug']}-esim?currency=USD"
            
            logger.info(f"ğŸŒ æ­£åœ¨æŠ“å– Airalo - {country['name']}...")
            
            self.driver.get(url)
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "button"))
            )
            
            buttons = self.driver.find_elements(By.TAG_NAME, "button")
            packages = []
            
            for btn in buttons:
                try:
                    hint = btn.get_attribute("hint") or btn.get_attribute("aria-label") or ""
                    if not hint or "USD" not in hint:
                        continue
                    
                    # è§£æ hint: "Select 1 GB - 3 days for $4.00 USD."
                    match = re.search(r'(\d+)\s*GB.*?(\d+)\s*days.*?\$?([\d.]+)\s*USD', hint, re.IGNORECASE)
                    if match:
                        data_gb = match.group(1)
                        validity_days = match.group(2)
                        price = float(match.group(3))
                        
                        package = {
                            "provider": "Airalo",
                            "country": country['name'],
                            "plan_name": f"{country['name']} {data_gb}GB {validity_days} Days",
                            "data_type": "Data",
                            "data_amount": f"{data_gb}GB",
                            "validity": f"{validity_days} Days",
                            "price": price,
                            "network": "Local Operators",
                            "link": url,
                            "raw_data": json.dumps({"hint": hint}),
                            "last_checked": datetime.utcnow().isoformat(),
                        }
                        packages.append(package)
                except:
                    continue
            
            if packages:
                logger.info(f"âœ… Airalo {country['name']}: è·å– {len(packages)} ä¸ªå¥—é¤")
                self.stats["airalo_scraped"] += len(packages)
                return packages
            else:
                raise Exception("æœªæŠ“å–åˆ°æ•°æ®")
                
        except Exception as e:
            logger.warning(f"âš ï¸ Airalo {country['name']}: æœªæŠ“å–åˆ°æ•°æ®ï¼Œå°è¯•ä½¿ç”¨æ—§æ•°æ®")
            old_packages = self.old_airalo_data.get(country['name'], [])
            if old_packages:
                logger.info(f"âœ… ä½¿ç”¨æ—§æ•°æ®: {len(old_packages)} ä¸ªå¥—é¤")
                self.stats["airalo_scraped"] += len(old_packages)
                return old_packages
            return []
    
    def scrape_nomad_country(self, country: Dict) -> List[Dict]:
        """ä½¿ç”¨ BeautifulSoup æŠ“å– Nomadï¼ˆç¨³å®šå¯é ï¼‰"""
        try:
            nomad_slug = country['nomad_slug'].replace('_', '-')
            url = f"https://www.getnomad.app/{nomad_slug}-esim"
            
            logger.info(f"ğŸŒ æ­£åœ¨æŠ“å– Nomad - {country['name']}...")
            
            response = self.session.get(url, timeout=15)
            
            if response.status_code != 200:
                logger.warning(f"âŒ Nomad {country['name']}: HTTP {response.status_code}")
                return []
            
            soup = BeautifulSoup(response.content, 'html.parser')
            packages = []
            
            plan_items = soup.find_all('li')
            
            for item in plan_items:
                text = item.get_text(strip=True)
                
                if 'USD' not in text:
                    continue
                
                text_clean = text.replace('Plan Details', '').strip()
                
                # åŒ¹é…æ•°æ®é‡
                data_match = re.search(r'(\d+)\s*GB|Unlimited', text_clean)
                if not data_match:
                    continue
                
                data_str = data_match.group(0)
                
                # åŒ¹é…æœ‰æ•ˆæœŸ
                validity_match = re.search(r'For\s+(\d+)\s*DAYS', text_clean, re.IGNORECASE)
                if not validity_match:
                    continue
                
                validity_days = validity_match.group(1)
                validity = f"{validity_days} Days"
                
                # åŒ¹é…ä»·æ ¼
                price_match = re.search(r'USD\s*([\d.]+)', text_clean)
                if not price_match:
                    continue
                
                price = float(price_match.group(1))
                
                is_unlimited = 'Unlimited' in data_str
                
                if is_unlimited:
                    data_amount = "Unlimited"
                    data_type = "Unlimited"
                else:
                    data_amount = data_str.replace(' GB', 'GB').strip()
                    data_type = "Data"
                
                package = {
                    "provider": "Nomad",
                    "country": country['name'],
                    "plan_name": f"{country['name']} {data_str.strip()} {validity}",
                    "data_type": data_type,
                    "data_amount": data_amount,
                    "validity": validity,
                    "price": price,
                    "network": "Local Operators",
                    "link": url,
                    "raw_data": json.dumps({
                        "currency": "USD",
                        "original_price": price,
                        "data": data_str.strip(),
                        "validity": validity,
                    }),
                    "last_checked": datetime.utcnow().isoformat(),
                }
                packages.append(package)
            
            logger.info(f"âœ… Nomad {country['name']}: è·å– {len(packages)} ä¸ªå¥—é¤")
            self.stats["nomad_scraped"] += len(packages)
            return packages
            
        except Exception as e:
            logger.error(f"âŒ Nomad {country['name']} é”™è¯¯: {str(e)}")
            return []
    
    def upsert_to_supabase(self, packages: List[Dict]) -> int:
        """Upsert æ•°æ®åˆ° Supabaseï¼ˆä½¿ç”¨ merge-duplicates ç­–ç•¥ï¼‰"""
        if not packages:
            return 0
        
        success_count = 0
        
        for pkg in packages:
            try:
                url = f"{SUPABASE_URL}/rest/v1/esim_packages"
                
                # ä½¿ç”¨ Prefer: resolution=merge-duplicates å®ç° Upsert
                # éœ€è¦æ•°æ®åº“æœ‰å”¯ä¸€é”®çº¦æŸï¼š(provider, country, plan_name)
                upsert_headers = {
                    **self.supabase_headers,
                    "Prefer": "resolution=merge-duplicates"
                }
                
                response = requests.post(
                    url,
                    headers=upsert_headers,
                    json=pkg,
                    timeout=10
                )
                
                if response.status_code in [200, 201]:
                    success_count += 1
                    self.stats["upsert_success"] += 1
                else:
                    logger.error(f"âŒ {pkg['provider']} - {pkg['country']} - {pkg['plan_name']}: å…¥åº“å¤±è´¥ ({response.status_code})")
                    self.stats["upsert_error"] += 1
                    
            except Exception as e:
                logger.error(f"âŒ {pkg['provider']} - {pkg['country']} - {pkg['plan_name']}: {str(e)[:100]}")
                self.stats["upsert_error"] += 1
        
        return success_count
    
    def run(self):
        """ä¸»è¿è¡Œæµç¨‹"""
        logger.info("=" * 70)
        logger.info("ğŸš€ GlobalPass æ··åˆçˆ¬è™«å¯åŠ¨ (ä¿®å¤ç‰ˆ)")
        logger.info("=" * 70)
        
        self.load_old_airalo_data()
        countries = self.load_countries()
        logger.info(f"ğŸ“‹ åŠ è½½ {len(countries)} ä¸ªå›½å®¶é…ç½®")
        
        for country in countries:
            logger.info("")
            logger.info("=" * 60)
            logger.info(f"ğŸŒ å¤„ç†å›½å®¶: {country['name']}")
            logger.info("=" * 60)
            
            all_packages = []
            
            # Airalo (Selenium + æ—§æ•°æ®å…œåº•)
            airalo_packages = self.scrape_airalo_country(country)
            all_packages.extend(airalo_packages)
            
            # Nomad (BeautifulSoup)
            nomad_packages = self.scrape_nomad_country(country)
            all_packages.extend(nomad_packages)
            
            # å…¥åº“
            if all_packages:
                self.upsert_to_supabase(all_packages)
        
        self.close_selenium()
        
        logger.info("")
        logger.info("ğŸ“Š çˆ¬è™«ç»Ÿè®¡")
        logger.info("=" * 70)
        logger.info(f"Airalo å¥—é¤: {self.stats['airalo_scraped']}")
        logger.info(f"Nomad å¥—é¤: {self.stats['nomad_scraped']}")
        logger.info(f"æ€»è®¡: {self.stats['airalo_scraped'] + self.stats['nomad_scraped']} ä¸ªå¥—é¤")
        logger.info(f"Upsert æˆåŠŸ: {self.stats['upsert_success']}, å¤±è´¥: {self.stats['upsert_error']}")

if __name__ == "__main__":
    scraper = UniversalScraper()
    scraper.run()
