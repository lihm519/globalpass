#!/usr/bin/env python3
"""
GlobalPass - é€šç”¨çˆ¬è™«æ ¸å¿ƒç³»ç»Ÿï¼ˆPlaywright ç‰ˆæœ¬ï¼‰
ä¿®å¤ï¼šAiralo ç½‘ç«™æ”¹ç‰ˆåä½¿ç”¨ JavaScript æ¸²æŸ“ï¼Œéœ€è¦ä½¿ç”¨æµè§ˆå™¨è‡ªåŠ¨åŒ–
åŠŸèƒ½ï¼š
- ä» Airalo å®˜ç½‘ç½‘é¡µæŠ“å–çœŸå®æ•°æ®ï¼ˆä½¿ç”¨ Playwrightï¼‰
- ä» Nomad å®˜ç½‘ç½‘é¡µæŠ“å–çœŸå®æ•°æ®ï¼ˆä½¿ç”¨ requestsï¼‰
- è´§å¸è½¬æ¢ï¼ˆEUR â†’ USDï¼‰
- æ— é™æµé‡è¯†åˆ«
- æœ‰æ•ˆæœŸæ¸…æ´—
- Upsert å…¥åº“
"""
import json
import requests
import logging
import os
import re
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional
from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError

# åˆ›å»ºæ—¥å¿—ç›®å½•
log_dir = Path("logs")
log_dir.mkdir(exist_ok=True)

# é…ç½®æ—¥å¿—ï¼ˆåŒæ—¶è¾“å‡ºåˆ°æ–‡ä»¶å’Œæ§åˆ¶å°ï¼‰
log_file = log_dir / f"scraper_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Supabase é…ç½®ï¼ˆä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ï¼‰
SUPABASE_URL = os.getenv("SUPABASE_URL", "https://mzodnvjtlujvvwfnpcyb.supabase.co")
SERVICE_ROLE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY", "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Im16b2Rudmp0bHVqdnZ3Zm5wY3liIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc2NzU0MDk4NiwiZXhwIjoyMDgzMTE2OTg2fQ.gr-5J22EhV08PLghNcoS8o5lUFjaEyby21MwE-35ENs")

class UniversalScraper:
    """é€šç”¨çˆ¬è™«ç±» - æ”¯æŒ Airalo (Playwright) å’Œ Nomad (requests)"""
    
    def __init__(self):
        self.supabase_headers = {
            "apikey": SERVICE_ROLE_KEY,
            "Authorization": f"Bearer {SERVICE_ROLE_KEY}",
            "Content-Type": "application/json",
        }
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        })
        self.stats = {
            "airalo_scraped": 0,
            "nomad_scraped": 0,
            "upsert_success": 0,
            "upsert_error": 0,
        }
    
    def load_countries(self) -> List[Dict]:
        """åŠ è½½å›½å®¶é…ç½®"""
        config_file = Path(__file__).parent.parent / "config" / "countries.json"
        
        if not config_file.exists():
            logger.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
            return []
        
        with open(config_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def scrape_airalo_country(self, country: Dict) -> List[Dict]:
        """ä» Airalo å®˜ç½‘æŠ“å–å•ä¸ªå›½å®¶çš„æ•°æ®ï¼ˆä½¿ç”¨ Playwrightï¼‰"""
        try:
            url = f"https://www.airalo.com/{country['airalo_slug']}-esim"
            
            logger.info(f"ğŸŒ æ­£åœ¨æŠ“å– Airalo - {country['name']}...")
            
            with sync_playwright() as p:
                # å¯åŠ¨æµè§ˆå™¨ï¼ˆæ— å¤´æ¨¡å¼ï¼‰
                browser = p.chromium.launch(headless=True)
                page = browser.new_page()
                
                # è®¿é—®é¡µé¢
                page.goto(url, wait_until="networkidle", timeout=30000)
                
                # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆï¼ˆç­‰å¾…ä»»æ„æŒ‰é’®å‡ºç°ï¼‰
                try:
                    page.wait_for_selector('button', timeout=10000)
                except PlaywrightTimeoutError:
                    logger.warning(f"âš ï¸ Airalo {country['name']}: é¡µé¢åŠ è½½è¶…æ—¶")
                    browser.close()
                    return []
                
                # è·å–æ‰€æœ‰æŒ‰é’®å…ƒç´ 
                buttons = page.query_selector_all('button')
                
                packages = []
                for button in buttons:
                    # å°è¯•è·å– aria-label å±æ€§ï¼ˆå¯èƒ½æ˜¯ hint æˆ– aria-labelï¼‰
                    aria_label = button.get_attribute('aria-label') or button.get_attribute('hint') or ''
                    
                    # è§£æ aria-label: "Select 1 GB - 3 days for $4.00 USD."
                    match = re.match(r'Select\s+(\d+)\s*GB\s*-\s*(\d+)\s*days?\s+for\s+\$?([\d.]+)\s*USD', aria_label, re.IGNORECASE)
                    
                    if match:
                        data_amount = match.group(1)
                        validity_days = match.group(2)
                        price_usd = float(match.group(3))
                        validity = f"{validity_days} Day" if validity_days == "1" else f"{validity_days} Days"
                        
                        package = {
                            "provider": "Airalo",
                            "country": country['name'],
                            "plan_name": f"{country['name']} {data_amount}GB {validity}",
                            "data_type": "Data",
                            "data_amount": f"{data_amount}GB",
                            "validity": validity,
                            "price": price_usd,
                            "network": "Local Operators",
                            "link": url,
                            "raw_data": json.dumps({
                                "original_price": price_usd,
                                "currency": "USD",
                                "data": f"{data_amount}GB",
                                "validity": validity,
                            }),
                            "last_checked": datetime.utcnow().isoformat(),
                        }
                        packages.append(package)
                        logger.debug(f"   âœ… {data_amount}GB {validity} - ${price_usd}")
                
                browser.close()
                
                logger.info(f"âœ… Airalo {country['name']}: è·å– {len(packages)} ä¸ªå¥—é¤")
                self.stats["airalo_scraped"] += len(packages)
                return packages
                
        except Exception as e:
            logger.error(f"âŒ Airalo {country['name']} é”™è¯¯: {str(e)[:200]}")
            return []
    
    def scrape_nomad_country(self, country: Dict) -> List[Dict]:
        """ä» Nomad å®˜ç½‘æŠ“å–å•ä¸ªå›½å®¶çš„æ•°æ®"""
        try:
            url = f"https://www.getnomad.app/api/v1/packages?country_slug={country['nomad_slug']}"
            
            logger.info(f"ğŸŒ æ­£åœ¨æŠ“å– Nomad - {country['name']}...")
            
            response = self.session.get(url, timeout=15)
            
            if response.status_code != 200:
                logger.warning(f"âŒ Nomad {country['name']}: HTTP {response.status_code}")
                return []
            
            data = response.json()
            packages = []
            
            if not data.get("data"):
                logger.warning(f"âš ï¸ Nomad {country['name']}: æ— æ•°æ®")
                return []
            
            for item in data["data"]:
                # è§£ææ•°æ®é‡
                data_amount = item.get("data", "Unknown")
                if data_amount == "unlimited":
                    data_type = "Unlimited"
                    data_amount_str = "Unlimited"
                else:
                    data_type = "Data"
                    data_amount_str = f"{data_amount}GB"
                
                # è§£ææœ‰æ•ˆæœŸ
                validity_days = item.get("validity", 0)
                validity = f"{validity_days} Day" if validity_days == 1 else f"{validity_days} Days"
                
                # ä»·æ ¼ï¼ˆNomad ä½¿ç”¨ USDï¼‰
                price = float(item.get("price", 0))
                
                # å¥—é¤åç§°
                plan_name = f"{country['name']} {data_amount_str} {validity}"
                
                package = {
                    "provider": "Nomad",
                    "country": country['name'],
                    "plan_name": plan_name,
                    "data_type": data_type,
                    "data_amount": data_amount_str,
                    "validity": validity,
                    "price": price,
                    "network": "Local Operators",
                    "link": f"https://www.getnomad.app/shop/{country['nomad_slug']}",
                    "raw_data": json.dumps({
                        "original_price": price,
                        "currency": "USD",
                        "data": data_amount,
                        "validity": validity,
                    }),
                    "last_checked": datetime.utcnow().isoformat(),
                }
                packages.append(package)
                logger.debug(f"   âœ… {data_amount_str} {validity} - ${price}")
            
            logger.info(f"âœ… Nomad {country['name']}: è·å– {len(packages)} ä¸ªå¥—é¤")
            self.stats["nomad_scraped"] += len(packages)
            return packages
            
        except Exception as e:
            logger.error(f"âŒ Nomad {country['name']} é”™è¯¯: {str(e)[:100]}")
            return []
    
    def upsert_package(self, package: Dict) -> bool:
        """Upsert å•ä¸ªå¥—é¤åˆ° Supabase"""
        try:
            url = f"{SUPABASE_URL}/rest/v1/esim_packages"
            
            # ä½¿ç”¨ provider + country + plan_name ä½œä¸ºå”¯ä¸€æ ‡è¯†
            params = {
                "provider": f"eq.{package['provider']}",
                "country": f"eq.{package['country']}",
                "plan_name": f"eq.{package['plan_name']}",
            }
            
            # å…ˆæŸ¥è¯¢æ˜¯å¦å­˜åœ¨
            response = self.session.get(url, headers=self.supabase_headers, params=params)
            
            if response.status_code == 200 and response.json():
                # å­˜åœ¨ï¼Œæ‰§è¡Œæ›´æ–°
                existing_id = response.json()[0]['id']
                update_url = f"{url}?id=eq.{existing_id}"
                response = self.session.patch(update_url, headers=self.supabase_headers, json=package)
            else:
                # ä¸å­˜åœ¨ï¼Œæ‰§è¡Œæ’å…¥
                response = self.session.post(url, headers=self.supabase_headers, json=package)
            
            if response.status_code in [200, 201]:
                logger.info(f"âœ… {package['provider']} - {package['country']} - {package['plan_name']}: å…¥åº“æˆåŠŸ (${package['price']})")
                self.stats["upsert_success"] += 1
                return True
            else:
                logger.error(f"âŒ {package['provider']} - {package['country']} - {package['plan_name']}: å…¥åº“å¤±è´¥ ({response.status_code})")
                self.stats["upsert_error"] += 1
                return False
                
        except Exception as e:
            logger.error(f"âŒ Upsert é”™è¯¯: {str(e)[:100]}")
            self.stats["upsert_error"] += 1
            return False
    
    def run(self):
        """è¿è¡Œçˆ¬è™«"""
        logger.info("=" * 70)
        logger.info("ğŸš€ GlobalPass é€šç”¨çˆ¬è™«å¯åŠ¨ (Playwright ç‰ˆæœ¬)")
        logger.info("=" * 70)
        
        countries = self.load_countries()
        
        if not countries:
            logger.error("âŒ æ— æ³•åŠ è½½å›½å®¶é…ç½®ï¼Œé€€å‡º")
            return
        
        logger.info(f"ğŸ“‹ åŠ è½½ {len(countries)} ä¸ªå›½å®¶é…ç½®")
        
        for country in countries:
            logger.info("")
            logger.info("=" * 60)
            logger.info(f"ğŸŒ å¤„ç†å›½å®¶: {country['name']}")
            logger.info("=" * 60)
            
            # æŠ“å– Airalo
            airalo_packages = self.scrape_airalo_country(country)
            
            # æŠ“å– Nomad
            nomad_packages = self.scrape_nomad_country(country)
            
            # åˆå¹¶æ‰€æœ‰å¥—é¤
            all_packages = airalo_packages + nomad_packages
            
            # Upsert åˆ°æ•°æ®åº“
            for package in all_packages:
                self.upsert_package(package)
        
        # æ‰“å°ç»Ÿè®¡
        logger.info("")
        logger.info("=" * 70)
        logger.info("ğŸ“Š çˆ¬è™«ç»Ÿè®¡")
        logger.info("=" * 70)
        logger.info(f"Airalo å¥—é¤: {self.stats['airalo_scraped']}")
        logger.info(f"Nomad å¥—é¤: {self.stats['nomad_scraped']}")
        logger.info(f"æ€»è®¡: {self.stats['airalo_scraped'] + self.stats['nomad_scraped']} ä¸ªå¥—é¤")
        logger.info(f"Upsert æˆåŠŸ: {self.stats['upsert_success']}, å¤±è´¥: {self.stats['upsert_error']}")
        logger.info("=" * 70)

if __name__ == "__main__":
    scraper = UniversalScraper()
    scraper.run()
