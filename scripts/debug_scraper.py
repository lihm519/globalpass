#!/usr/bin/env python3
"""
è°ƒè¯•è„šæœ¬ï¼šæŸ¥çœ‹ Airalo å’Œ Nomad ç½‘é¡µçš„å®é™…æ–‡æœ¬æ ¼å¼
"""
import requests
from bs4 import BeautifulSoup

def debug_airalo():
    """è°ƒè¯• Airalo ç½‘é¡µ"""
    url = "https://www.airalo.com/japan-esim?currency=USD"
    
    print(f"ğŸŒ è®¿é—® Airalo: {url}\n")
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }
    
    response = requests.get(url, headers=headers, timeout=15)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
    links = soup.find_all('a')
    
    print("=" * 60)
    print("Airalo é“¾æ¥æ–‡æœ¬ï¼ˆå‰ 50 ä¸ªï¼‰:")
    print("=" * 60)
    
    for i, link in enumerate(links[:50]):
        text = link.get_text(strip=True)
        if text and ('GB' in text or 'USD' in text or '$' in text):
            print(f"{i+1}. [{text}]")
    
    print("\n")

def debug_nomad():
    """è°ƒè¯• Nomad ç½‘é¡µ"""
    url = "https://www.getnomad.app/japan-esim"
    
    print(f"ğŸŒ è®¿é—® Nomad: {url}\n")
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }
    
    response = requests.get(url, headers=headers, timeout=15)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # æŸ¥æ‰¾æ‰€æœ‰ <li> å…ƒç´ 
    items = soup.find_all('li')
    
    print("=" * 60)
    print("Nomad <li> æ–‡æœ¬ï¼ˆå‰ 50 ä¸ªï¼‰:")
    print("=" * 60)
    
    for i, item in enumerate(items[:50]):
        text = item.get_text(strip=True)
        if text and ('GB' in text or 'USD' in text or 'Days' in text.upper()):
            print(f"{i+1}. [{text}]")
    
    print("\n")

if __name__ == "__main__":
    debug_airalo()
    debug_nomad()
