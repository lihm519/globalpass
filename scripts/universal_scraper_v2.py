#!/usr/bin/env python3
"""
GlobalPass - é€šç”¨çˆ¬è™«æ ¸å¿ƒç³»ç»Ÿï¼ˆç½‘é¡µæŠ“å–ç‰ˆ v2ï¼‰
åŠŸèƒ½ï¼š
- ä» Airalo å®˜ç½‘æŠ“å–æ¬§å…ƒä»·æ ¼å¹¶è½¬æ¢ä¸ºç¾å…ƒ
- ä» Nomad å®˜ç½‘æŠ“å–æ–°åŠ å¡å…ƒä»·æ ¼å¹¶è½¬æ¢ä¸ºç¾å…ƒ
- ç»Ÿä¸€å‰ç«¯æ˜¾ç¤ºç¾å…ƒä»·æ ¼
"""
import json
import requests
import logging
from datetime import datetime
from pathlib import Path
from typing import List, Dict
import re
from bs4 import BeautifulSoup

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Supabase é…ç½®
SUPABASE_URL = "https://mzodnvjtlujvvwfnpcyb.supabase.co"
SERVICE_ROLE_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Im16b2Rudmp0bHVqdnZ3Zm5wY3liIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc2NzU0MDk4NiwiZXhwIjoyMDgzMTE2OTg2fQ.gr-5J22EhV08PLghNcoS8o5lUFjaEyby21MwE-35ENs"

# æ±‡ç‡é…ç½®ï¼ˆ2026å¹´1æœˆ5æ—¥ï¼‰
EXCHANGE_RATES = {
    "EUR": 1.17,  # 1 EUR = 1.17 USD
    "SGD": 0.78,  # 1 SGD = 0.78 USD
    "USD": 1.00,  # 1 USD = 1.00 USD
    "CNY": 0.14,  # 1 CNY = 0.14 USD
    "GBP": 1.27,  # 1 GBP = 1.27 USD
}

class UniversalScraper:
    """é€šç”¨çˆ¬è™«ç±» - æ”¯æŒ Airalo å’Œ Nomad"""
    
    def __init__(self):
        self.supabase_headers = {
            "apikey": SERVICE_ROLE_KEY,
            "Authorization": f"Bearer {SERVICE_ROLE_KEY}",
            "Content-Type": "application/json",
        }
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        })
        self.stats = {
            "airalo_scraped": 0,
            "nomad_scraped": 0,
            "upsert_success": 0,
            "upsert_error": 0,
        }
    
    def convert_to_usd(self, price: float, currency: str) -> float:
        """å°†ä»·æ ¼è½¬æ¢ä¸ºç¾å…ƒ"""
        rate = EXCHANGE_RATES.get(currency, 1.0)
        return round(price * rate, 2)
    
    def load_countries(self) -> List[Dict]:
        """åŠ è½½å›½å®¶é…ç½®"""
        config_file = Path(__file__).parent.parent / "config" / "countries.json"
        
        if not config_file.exists():
            logger.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
            return []
        
        with open(config_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def scrape_airalo_country(self, country: Dict) -> List[Dict]:
        """ä» Airalo å®˜ç½‘æŠ“å–å•ä¸ªå›½å®¶çš„æ•°æ®"""
        try:
            url = f"https://www.airalo.com/{country['airalo_slug']}-esim"
            
            logger.info(f"ğŸŒ æ­£åœ¨æŠ“å– Airalo - {country['name']}...")
            
            response = self.session.get(url, timeout=15)
            
            if response.status_code != 200:
                logger.warning(f"âŒ Airalo {country['name']}: HTTP {response.status_code}")
                return []
            
            soup = BeautifulSoup(response.content, 'html.parser')
            packages = []
            
            # æŸ¥æ‰¾æ‰€æœ‰å¥—é¤é“¾æ¥
            package_links = soup.find_all('a')
            
            current_validity = "7 Days"  # é»˜è®¤æœ‰æ•ˆæœŸ
            
            for link in package_links:
                text = link.get_text(strip=True)
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆæœŸæ ‡ç­¾
                if re.match(r'^\d+\s*days?$', text, re.IGNORECASE):
                    current_validity = text.replace('days', 'Days').replace('day', 'Day')
                    continue
                
                # è§£ææ ‡å‡†å¥—é¤: "1GB4.00 â‚¬"
                standard_match = re.match(r'^(\d+)\s*GB([\d.]+)\s*â‚¬$', text)
                
                if standard_match:
                    data_amount = standard_match.group(1)
                    price_eur = float(standard_match.group(2))
                    price_usd = self.convert_to_usd(price_eur, "EUR")
                    
                    package = {
                        "provider": "Airalo",
                        "country": country['name'],
                        "plan_name": f"{country['name']} {data_amount}GB {current_validity}",
                        "data_type": "Data",
                        "data_amount": f"{data_amount}GB",
                        "validity": current_validity,
                        "price": price_usd,
                        "network": "Local Operators",
                        "link": f"https://www.airalo.com/{country['airalo_slug']}-esim",
                        "raw_data": json.dumps({
                            "original_price": price_eur,
                            "original_currency": "EUR",
                            "usd_price": price_usd,
                            "currency": "USD",
                            "data": f"{data_amount}GB",
                            "validity": current_validity,
                        }),
                        "last_checked": datetime.utcnow().isoformat(),
                    }
                    packages.append(package)
                    logger.debug(f"   âœ… æ ‡å‡†å¥—é¤: {data_amount}GB â‚¬{price_eur} â†’ ${price_usd}")
                
                # è§£ææ— é™æµé‡å¥—é¤: "Unlimited7.50 â‚¬"
                unlimited_match = re.match(r'^Unlimited(?:Data)?([\d.]+)\s*â‚¬$', text)
                
                if unlimited_match:
                    price_eur = float(unlimited_match.group(1))
                    price_usd = self.convert_to_usd(price_eur, "EUR")
                    
                    package = {
                        "provider": "Airalo",
                        "country": country['name'],
                        "plan_name": f"{country['name']} Unlimited {current_validity}",
                        "data_type": "Unlimited",
                        "data_amount": "Unlimited",
                        "validity": current_validity,
                        "price": price_usd,
                        "network": "Local Operators",
                        "link": f"https://www.airalo.com/{country['airalo_slug']}-esim",
                        "raw_data": json.dumps({
                            "original_price": price_eur,
                            "original_currency": "EUR",
                            "usd_price": price_usd,
                            "currency": "USD",
                            "data": "Unlimited",
                            "validity": current_validity,
                        }),
                        "last_checked": datetime.utcnow().isoformat(),
                    }
                    packages.append(package)
                    logger.debug(f"   âœ… æ— é™å¥—é¤: Unlimited â‚¬{price_eur} â†’ ${price_usd}")
            
            logger.info(f"âœ… Airalo {country['name']}: è·å– {len(packages)} ä¸ªå¥—é¤")
            self.stats["airalo_scraped"] += len(packages)
            return packages
            
        except Exception as e:
            logger.error(f"âŒ Airalo {country['name']} é”™è¯¯: {str(e)[:100]}")
            return []
    
    def scrape_nomad_country(self, country: Dict) -> List[Dict]:
        """ä» Nomad å®˜ç½‘æŠ“å–å•ä¸ªå›½å®¶çš„æ•°æ®"""
        try:
            nomad_slug = country['nomad_slug'].replace('_', '-')
            url = f"https://www.getnomad.app/{nomad_slug}-esim"
            
            logger.info(f"ğŸŒ æ­£åœ¨æŠ“å– Nomad - {country['name']}...")
            
            response = self.session.get(url, timeout=15)
            
            if response.status_code != 200:
                logger.warning(f"âŒ Nomad {country['name']}: HTTP {response.status_code}")
                return []
            
            soup = BeautifulSoup(response.content, 'html.parser')
            packages = []
            
            # æŸ¥æ‰¾æ‰€æœ‰å¥—é¤ <li> å…ƒç´ 
            plan_items = soup.find_all('li')
            
            for item in plan_items:
                text = item.get_text(strip=True)
                
                # è·³è¿‡ä¸åŒ…å«å¸ç§çš„é¡¹ç›®
                if 'USD' not in text and 'SGD' not in text and 'EUR' not in text:
                    continue
                
                # ç§»é™¤ "Plan Details" å‰ç¼€
                text_clean = text.replace('Plan Details', '').strip()
                
                # åŒ¹é…æ•°æ®é‡
                data_match = re.search(r'(\d+)\s*GB|Unlimited', text_clean)
                if not data_match:
                    continue
                
                data_str = data_match.group(0)
                
                # åŒ¹é…æœ‰æ•ˆæœŸ
                validity_match = re.search(r'For\s+(\d+)\s*DAYS', text_clean, re.IGNORECASE)
                if not validity_match:
                    continue
                
                validity_days = validity_match.group(1)
                validity = f"{validity_days} Days"
                
                # åŒ¹é…ä»·æ ¼ (USD/SGD/EUR)
                price_match = re.search(r'(USD|SGD|EUR)\s*([\d.]+)', text_clean)
                if not price_match:
                    continue
                
                currency = price_match.group(1)
                price_value = float(price_match.group(2))
                
                # è½¬æ¢ä¸ºç¾å…ƒ
                price_usd = self.convert_to_usd(price_value, currency)
                
                # åˆ¤æ–­æ˜¯å¦æ˜¯æ— é™æµé‡
                is_unlimited = 'Unlimited' in data_str
                
                if is_unlimited:
                    data_amount = "Unlimited"
                    data_type = "Unlimited"
                else:
                    data_amount = data_str.replace(' GB', 'GB').strip()
                    data_type = "Data"
                
                package = {
                    "provider": "Nomad",
                    "country": country['name'],
                    "plan_name": f"{country['name']} {data_str.strip()} {validity}",
                    "data_type": data_type,
                    "data_amount": data_amount,
                    "validity": validity,
                    "price": price_usd,
                    "network": "Local Operators",
                    "link": f"https://www.getnomad.app/{nomad_slug}-esim",
                    "raw_data": json.dumps({
                        "original_currency": currency,
                        "original_price": price_value,
                        "usd_price": price_usd,
                        "currency": "USD",
                        "data": data_str.strip(),
                        "validity": validity,
                    }),
                    "last_checked": datetime.utcnow().isoformat(),
                }
                packages.append(package)
                logger.debug(f"   âœ… å¥—é¤: {data_str.strip()} {validity} {currency}{price_value} â†’ ${price_usd}")
            
            logger.info(f"âœ… Nomad {country['name']}: è·å– {len(packages)} ä¸ªå¥—é¤")
            self.stats["nomad_scraped"] += len(packages)
            return packages
            
        except Exception as e:
            logger.error(f"âŒ Nomad {country['name']} é”™è¯¯: {str(e)[:100]}")
            return []
    
    def upsert_to_supabase(self, packages: List[Dict]) -> int:
        """Upsert æ•°æ®åˆ° Supabase"""
        if not packages:
            return 0
        
        success_count = 0
        
        for pkg in packages:
            try:
                url = f"{SUPABASE_URL}/rest/v1/esim_packages"
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                check_url = f"{url}?provider=eq.{pkg['provider']}&country=eq.{pkg['country']}&plan_name=eq.{pkg['plan_name']}"
                
                check_response = requests.get(
                    check_url,
                    headers=self.supabase_headers,
                    timeout=10
                )
                
                existing = check_response.json()
                
                if existing:
                    # æ›´æ–°
                    record_id = existing[0]['id']
                    update_url = f"{url}?id=eq.{record_id}"
                    
                    response = requests.patch(
                        update_url,
                        headers=self.supabase_headers,
                        json=pkg,
                        timeout=10
                    )
                else:
                    # æ’å…¥
                    response = requests.post(
                        url,
                        headers=self.supabase_headers,
                        json=pkg,
                        timeout=10
                    )
                
                if response.status_code in [200, 201]:
                    success_count += 1
                    self.stats["upsert_success"] += 1
                else:
                    logger.warning(f"âš ï¸  Upsert å¤±è´¥: {pkg['plan_name']} - {response.status_code}")
                    self.stats["upsert_error"] += 1
                    
            except Exception as e:
                logger.error(f"âŒ Upsert é”™è¯¯: {pkg['plan_name']} - {str(e)[:50]}")
                self.stats["upsert_error"] += 1
        
        return success_count
    
    def run(self):
        """è¿è¡Œçˆ¬è™«"""
        logger.info("ğŸš€ GlobalPass é€šç”¨çˆ¬è™«å¯åŠ¨...")
        logger.info(f"ğŸ“… æ±‡ç‡æ›´æ–°æ—¶é—´: 2026-01-05")
        logger.info(f"ğŸ’± EURâ†’USD: {EXCHANGE_RATES['EUR']}, SGDâ†’USD: {EXCHANGE_RATES['SGD']}\n")
        
        countries = self.load_countries()
        logger.info(f"ğŸ“ ç›®æ ‡å›½å®¶: {len(countries)} ä¸ª\n")
        
        all_packages = []
        
        for country in countries:
            logger.info("=" * 60)
            logger.info(f"ğŸŒ å¤„ç†å›½å®¶: {country['name']}")
            logger.info("=" * 60)
            
            # æŠ“å– Airalo
            airalo_packages = self.scrape_airalo_country(country)
            all_packages.extend(airalo_packages)
            
            # æŠ“å– Nomad
            nomad_packages = self.scrape_nomad_country(country)
            all_packages.extend(nomad_packages)
        
        # Upsert åˆ°æ•°æ®åº“
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“¤ æ­£åœ¨æ›´æ–°æ•°æ®åº“...")
        logger.info("=" * 60)
        
        success_count = self.upsert_to_supabase(all_packages)
        
        # ç»Ÿè®¡æŠ¥å‘Š
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“Š çˆ¬è™«ç»Ÿè®¡æŠ¥å‘Š")
        logger.info("=" * 60)
        logger.info(f"âœ… Airalo æŠ“å–: {self.stats['airalo_scraped']} ä¸ªå¥—é¤")
        logger.info(f"âœ… Nomad æŠ“å–: {self.stats['nomad_scraped']} ä¸ªå¥—é¤")
        logger.info(f"âœ… æ•°æ®åº“æ›´æ–°æˆåŠŸ: {self.stats['upsert_success']} æ¡")
        logger.info(f"âŒ æ•°æ®åº“æ›´æ–°å¤±è´¥: {self.stats['upsert_error']} æ¡")
        logger.info("=" * 60)

if __name__ == "__main__":
    scraper = UniversalScraper()
    scraper.run()
